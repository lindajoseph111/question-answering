{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data and take a quick look\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from gensim.test.utils import datapath\n",
    "from gensim.models import KeyedVectors\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>guid</th>\n",
       "      <th>qu_id</th>\n",
       "      <th>qu</th>\n",
       "      <th>qa_id</th>\n",
       "      <th>qa</th>\n",
       "      <th>ans_id</th>\n",
       "      <th>ans</th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>is_correspond</th>\n",
       "      <th>is_useful</th>\n",
       "      <th>f_uq</th>\n",
       "      <th>f_aq</th>\n",
       "      <th>l_uq</th>\n",
       "      <th>l_aq</th>\n",
       "      <th>num_words_uq</th>\n",
       "      <th>num_words_aq</th>\n",
       "      <th>n_common_words</th>\n",
       "      <th>total_word_count</th>\n",
       "      <th>fraction_common_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>98550</td>\n",
       "      <td>198647</td>\n",
       "      <td>is lamborghini better than ferrari</td>\n",
       "      <td>198648</td>\n",
       "      <td>why is lamborghini better than ferrari</td>\n",
       "      <td>73400</td>\n",
       "      <td>Lamborghinis.Thats why I have 24 of them compa...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>36</td>\n",
       "      <td>40</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>6.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.461538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>23880</td>\n",
       "      <td>122782</td>\n",
       "      <td>what does a trump presidency mean for indian s...</td>\n",
       "      <td>12385</td>\n",
       "      <td>how would trump presidency affect indian stude...</td>\n",
       "      <td>76886</td>\n",
       "      <td>In the last Presidential debate, the conservat...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>97</td>\n",
       "      <td>61</td>\n",
       "      <td>17</td>\n",
       "      <td>11</td>\n",
       "      <td>6.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.214286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12</td>\n",
       "      <td>108664</td>\n",
       "      <td>210485</td>\n",
       "      <td>why is north america considered the west</td>\n",
       "      <td>210486</td>\n",
       "      <td>is north america considered the west</td>\n",
       "      <td>98240</td>\n",
       "      <td>The British invented the Prime Meridian.  They...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>56</td>\n",
       "      <td>52</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>8.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.470588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13</td>\n",
       "      <td>106306</td>\n",
       "      <td>181906</td>\n",
       "      <td>if you could live in a movie what movie would ...</td>\n",
       "      <td>201042</td>\n",
       "      <td>if you could enter the reality inside a movie ...</td>\n",
       "      <td>74909</td>\n",
       "      <td>Midnight in Paris..!Fantasising about living i...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>63</td>\n",
       "      <td>111</td>\n",
       "      <td>16</td>\n",
       "      <td>23</td>\n",
       "      <td>10.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.303030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15</td>\n",
       "      <td>103382</td>\n",
       "      <td>518790</td>\n",
       "      <td>what jobs are in art stream</td>\n",
       "      <td>518791</td>\n",
       "      <td>what are scopes of arts stream</td>\n",
       "      <td>129264</td>\n",
       "      <td>You can opt anything as your carrier other tha...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>32</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>4.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.285714</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0    guid   qu_id  \\\n",
       "0           4   98550  198647   \n",
       "1           8   23880  122782   \n",
       "2          12  108664  210485   \n",
       "3          13  106306  181906   \n",
       "4          15  103382  518790   \n",
       "\n",
       "                                                  qu   qa_id  \\\n",
       "0                 is lamborghini better than ferrari  198648   \n",
       "1  what does a trump presidency mean for indian s...   12385   \n",
       "2           why is north america considered the west  210486   \n",
       "3  if you could live in a movie what movie would ...  201042   \n",
       "4                        what jobs are in art stream  518791   \n",
       "\n",
       "                                                  qa  ans_id  \\\n",
       "0             why is lamborghini better than ferrari   73400   \n",
       "1  how would trump presidency affect indian stude...   76886   \n",
       "2               is north america considered the west   98240   \n",
       "3  if you could enter the reality inside a movie ...   74909   \n",
       "4                     what are scopes of arts stream  129264   \n",
       "\n",
       "                                                 ans  is_duplicate  \\\n",
       "0  Lamborghinis.Thats why I have 24 of them compa...             1   \n",
       "1  In the last Presidential debate, the conservat...             1   \n",
       "2  The British invented the Prime Meridian.  They...             1   \n",
       "3  Midnight in Paris..!Fantasising about living i...             1   \n",
       "4  You can opt anything as your carrier other tha...             1   \n",
       "\n",
       "   is_correspond  is_useful  f_uq  f_aq  l_uq  l_aq  num_words_uq  \\\n",
       "0              1          1     1     1    36    40             6   \n",
       "1              1          1     2    17    97    61            17   \n",
       "2              1          1     1     1    56    52            10   \n",
       "3              1          1     2     2    63   111            16   \n",
       "4              1          1     1     1    29    32             7   \n",
       "\n",
       "   num_words_aq  n_common_words  total_word_count  fraction_common_words  \n",
       "0             7             6.0              13.0               0.461538  \n",
       "1            11             6.0              28.0               0.214286  \n",
       "2             9             8.0              17.0               0.470588  \n",
       "3            23            10.0              33.0               0.303030  \n",
       "4             7             4.0              14.0               0.285714  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QApair_df = pd.read_csv('train.csv')\n",
    "QApair_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all text size 8000\n",
      "All label size 8000\n"
     ]
    }
   ],
   "source": [
    "all_text = QApair_df[[\"qu\",\"qa\"]]\n",
    "all_labels = QApair_df[\"is_duplicate\"]\n",
    "labels_list = [1,0]\n",
    "print(\"all text size\", len(all_text))\n",
    "print(\"All label size\", len(all_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of train set 4800\n",
      "Size of the test set 3200\n",
      "2400\n",
      "1600\n"
     ]
    }
   ],
   "source": [
    "#splitting the training and test set with balanced classes\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_text, test_text, train_labels, test_labels = train_test_split(all_text, all_labels, test_size=0.4, random_state=0, stratify=all_labels)\n",
    "print(\"Size of train set\", len(train_text))\n",
    "print(\"Size of the test set\", len(test_text))\n",
    "print(sum(train_labels))\n",
    "print(sum(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_user_text = train_text[\"qu\"].tolist() #list of user questions for training\n",
    "test_user_text = test_text[\"qu\"].tolist() #list of user questions for testing\n",
    "\n",
    "train_arch_text = train_text[\"qa\"].tolist() #list of archived questions for training\n",
    "test_arch_text = test_text[\"qa\"].tolist() #list of archived questions for testing\n",
    "\n",
    "train_labels = train_labels.tolist() #list of training labels\n",
    "test_labels = test_labels.tolist() #list of test labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "# here I load the 300-dimension vectors; loading longer embeddings would require longer time and more RAM\n",
    "path_of_downloaded_files = \"C:\\\\Users\\\\ASUS\\\\NLP\\\\embeddings\\\\glove.6B\\\\glove.6B.300d.txt\" \n",
    "\n",
    "glove_file = datapath(path_of_downloaded_files)\n",
    "word2vec_glove_file = get_tmpfile(\"glove.6B.300d.txt\")\n",
    "glove2word2vec(glove_file, word2vec_glove_file)\n",
    "word_vectors = KeyedVectors.load_word2vec_format(word2vec_glove_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#building the simaese network\n",
    "# then we define the RNN-based classifier\n",
    "\n",
    "class RNN_Classifier(nn.Module):\n",
    "    def __init__(self, embd_dim, hidden_dim, model_type, cls_num, pooler_type, dropout, gpu):\n",
    "        super(RNN_Classifier, self).__init__()\n",
    "        assert pooler_type in ['max','avg']\n",
    "        # rnn type\n",
    "\n",
    "        self.rnn = nn.LSTM(hidden_size=hidden_dim, batch_first=True, input_size=embd_dim, dropout=dropout) \n",
    "\n",
    "        self.gpu = gpu\n",
    "        if gpu: self.to('cuda')\n",
    "            \n",
    "    def manhattan_distance(self, u, q):\n",
    "        ''' Helper function for the similarity estimate of the LSTMs outputs '''\n",
    "        return torch.exp(-torch.sum(torch.abs(u - q), dim=1))\n",
    "            \n",
    "    def forward(self, input_matrix_u,input_matrix_a):\n",
    "        token_num = input_matrix_u.shape[1]\n",
    "        hidden_vecs_u = self.rnn(input_matrix_u)[0]\n",
    "        #print('hidden_vecs_u',hidden_vecs_u.shape)\n",
    "        hidden_vecs_a = self.rnn(input_matrix_a)[0]\n",
    "        #print('hidden_vecs_a',hidden_vecs_a.shape)\n",
    "        #print(hidden_vecs_u[0].shape)\n",
    "        similarity_measure =self.manhattan_distance(hidden_vecs_u.permute(1, 2, 0).view(1, -1),hidden_vecs_a.permute(1, 2, 0).view(1, -1))\n",
    "        #print('sim sore**********',similarity_measure)        \n",
    "        return similarity_measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define functions that build mini-batches\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "\n",
    "embd_dim = 300\n",
    "hidden_dim = 300\n",
    "rnn_type = 'lstm'\n",
    "pooler_type = 'avg'\n",
    "dropout = 0.5\n",
    "gpu = True\n",
    "\n",
    "oov_vec = oov_vec = np.zeros(embd_dim)\n",
    "\n",
    "def get_sent_word_vecs(word_vectors, sent_words, largest_len):\n",
    "    vecs = []\n",
    "    #print(sent_words)\n",
    "    for ww in sent_words:\n",
    "        if ww in word_vectors:\n",
    "            vecs.append(word_vectors[ww])\n",
    "        else:\n",
    "            if ww == 0:oov_vec = np.zeros(embd_dim) #if word is padded zero append zero vector to it\n",
    "            else:oov_vec = np.zeros(embd_dim) #if word is not vocabulary of word2vec then append vector with values from normal distribution\n",
    "            vecs.append(oov_vec)\n",
    "    return np.array(vecs)\n",
    "\n",
    "def build_mini_batch(tokenized_sents, word_vectors,largest_len):\n",
    "    #print(\"build_mini_batch\")\n",
    "    text_vecs = []\n",
    "    for ts in tokenized_sents:\n",
    "        ts.extend([0] * (largest_len-len(ts))) #padding list with zeros to make all tensors the same size\n",
    "        vv = get_sent_word_vecs(word_vectors, ts, largest_len)\n",
    "        text_vecs.append(vv)\n",
    "    # print('mini batch shape',np.array(text_vecs).shape)\n",
    "    return np.array(text_vecs)\n",
    "\n",
    "def make_batch_prediction(sent_list_1,sent_list_2, word_vectors, model, use_gpu=False):\n",
    "    tokenized_sents_1 = [word_tokenize(ss.lower()) for ss in sent_list_1]\n",
    "    #print(tokenized_sents_1)\n",
    "    tokenized_sents_2 = [word_tokenize(ss.lower()) for ss in sent_list_2]\n",
    "    #print(tokenized_sents_2)\n",
    "    largest_len_1 = np.max([len(tokens) for tokens in tokenized_sents_1])\n",
    "    largest_len_2 = np.max([len(tokens) for tokens in tokenized_sents_2])\n",
    "    largest_len = max(largest_len_1,largest_len_2) #getting the largest length of seuences, so that shorter sentences can be padded later\n",
    "    #print(largest_len)\n",
    "    batch_u = build_mini_batch(tokenized_sents_1, word_vectors,largest_len) #build minibatch of vectors for userquestions\n",
    "    batch_a = build_mini_batch(tokenized_sents_2, word_vectors,largest_len) #build minibatch of vectors for archived questions\n",
    "    \n",
    "    batch_simscore = torch.tensor([])\n",
    "    if use_gpu: batch_simscore = batch_simscore.to('cuda')\n",
    "    for i in range(batch_u.shape[0]): #iteraing over each row of batch 1\n",
    "        input_sents_u = torch.from_numpy(batch_u[i]).float()\n",
    "        input_sents_a = torch.from_numpy(batch_a[i]).float()\n",
    "        if use_gpu: input_sents_u = input_sents_u.to('cuda')\n",
    "        if use_gpu: input_sents_a = input_sents_a.to('cuda')\n",
    "        similarity_score = model(input_sents_u.unsqueeze(0),input_sents_a.unsqueeze(0))\n",
    "        #print(similarity_score)\n",
    "        batch_simscore = torch.cat( (batch_simscore, similarity_score) )\n",
    "    return batch_simscore.view(batch_u.shape[0],-1)\n",
    "  \n",
    "# sanity check \n",
    "#model = RNN_Classifier(embd_dim, hidden_dim, rnn_type, len(labels_list), pooler_type, dropout, gpu)\n",
    "#batch_pred = make_batch_prediction(\n",
    "    #['hello world!','hello','another test sentence this is'],['lamborgini ferrari','hello','another test sentence this is'],\n",
    "    #word_vectors, model, gpu)\n",
    "#print(batch_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\Anaconda3\\envs\\venv_nlp\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:60: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "loss_fnc = torch.nn.MSELoss() # Mean squared Error loss\n",
    "\n",
    "# hyper parameters\n",
    "n_epochs = 20 # number of epoch (i.e. number of iterations)\n",
    "batch_size = 50\n",
    "lr = 0.001 # initial learning rate\n",
    "\n",
    "# init optimizer and scheduler (lr adjustor)\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "model = RNN_Classifier(embd_dim, hidden_dim, rnn_type, len(labels_list), pooler_type, dropout, gpu)\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=lr) # use Adam as the optimizer\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.999) # after each epoch, the learning rate is discounted to its 95%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======epoch 0 loss====== 0.49895763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  5%|████▏                                                                              | 1/20 [00:44<14:08, 44.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 0 the mean squared error dev set is 0.4990624847123047\n",
      "learning rate 0.001\n",
      "best model updated; new best MSE 0.4990624847123047\n",
      "\n",
      "======epoch 1 loss====== 0.49895784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|████████▎                                                                          | 2/20 [01:28<13:20, 44.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 1 the mean squared error dev set is 0.4990614651447733\n",
      "learning rate 0.000999\n",
      "best model updated; new best MSE 0.4990614651447733\n",
      "\n",
      "======epoch 2 loss====== 0.49869022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 15%|████████████▍                                                                      | 3/20 [02:12<12:31, 44.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 2 the mean squared error dev set is 0.4980785161774903\n",
      "learning rate 0.000998001\n",
      "best model updated; new best MSE 0.4980785161774903\n",
      "\n",
      "======epoch 3 loss====== 0.48144308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|████████████████▌                                                                  | 4/20 [02:55<11:42, 43.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 3 the mean squared error dev set is 0.42875435433037185\n",
      "learning rate 0.000997002999\n",
      "best model updated; new best MSE 0.42875435433037185\n",
      "\n",
      "======epoch 4 loss====== 0.3323079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|████████████████████▊                                                              | 5/20 [03:43<11:17, 45.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 4 the mean squared error dev set is 0.27022715679110737\n",
      "learning rate 0.000996005996001\n",
      "best model updated; new best MSE 0.27022715679110737\n",
      "\n",
      "======epoch 5 loss====== 0.25913814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|████████████████████████▉                                                          | 6/20 [04:29<10:35, 45.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 5 the mean squared error dev set is 0.23732654310641638\n",
      "learning rate 0.000995009990004999\n",
      "best model updated; new best MSE 0.23732654310641638\n",
      "\n",
      "======epoch 6 loss====== 0.22681093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 35%|█████████████████████████████                                                      | 7/20 [05:16<09:56, 45.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 6 the mean squared error dev set is 0.2269143962652299\n",
      "learning rate 0.000994014980014994\n",
      "best model updated; new best MSE 0.2269143962652299\n",
      "\n",
      "======epoch 7 loss====== 0.21347398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|█████████████████████████████████▏                                                 | 8/20 [06:04<09:18, 46.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 7 the mean squared error dev set is 0.22408986536751654\n",
      "learning rate 0.0009930209650349789\n",
      "best model updated; new best MSE 0.22408986536751654\n",
      "\n",
      "======epoch 8 loss====== 0.20488067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 45%|█████████████████████████████████████▎                                             | 9/20 [06:51<08:33, 46.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 8 the mean squared error dev set is 0.22275727428009837\n",
      "learning rate 0.0009920279440699439\n",
      "best model updated; new best MSE 0.22275727428009837\n",
      "\n",
      "======epoch 9 loss====== 0.19731879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████████████████████████████████████████                                         | 10/20 [07:37<07:44, 46.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 9 the mean squared error dev set is 0.2229650206055616\n",
      "learning rate 0.0009910359161258739\n",
      "\n",
      "======epoch 10 loss====== 0.1899092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 55%|█████████████████████████████████████████████                                     | 11/20 [08:21<06:50, 45.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 10 the mean squared error dev set is 0.22273312900793318\n",
      "learning rate 0.000990044880209748\n",
      "best model updated; new best MSE 0.22273312900793318\n",
      "\n",
      "======epoch 11 loss====== 0.18373065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|█████████████████████████████████████████████████▏                                | 12/20 [09:03<05:56, 44.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 11 the mean squared error dev set is 0.22193033538651016\n",
      "learning rate 0.0009890548353295382\n",
      "best model updated; new best MSE 0.22193033538651016\n",
      "\n",
      "======epoch 12 loss====== 0.17761366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 65%|█████████████████████████████████████████████████████▎                            | 13/20 [09:45<05:06, 43.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 12 the mean squared error dev set is 0.2211352936530019\n",
      "learning rate 0.0009880657804942088\n",
      "best model updated; new best MSE 0.2211352936530019\n",
      "\n",
      "======epoch 13 loss====== 0.17210822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|█████████████████████████████████████████████████████████▍                        | 14/20 [10:27<04:19, 43.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 13 the mean squared error dev set is 0.22092627986802868\n",
      "learning rate 0.0009870777147137145\n",
      "best model updated; new best MSE 0.22092627986802868\n",
      "\n",
      "======epoch 14 loss====== 0.16672106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|█████████████████████████████████████████████████████████████▌                    | 15/20 [11:09<03:34, 42.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 14 the mean squared error dev set is 0.22026232280461724\n",
      "learning rate 0.0009860906369990009\n",
      "best model updated; new best MSE 0.22026232280461724\n",
      "\n",
      "======epoch 15 loss====== 0.16118886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|█████████████████████████████████████████████████████████████████▌                | 16/20 [11:51<02:50, 42.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 15 the mean squared error dev set is 0.22057397705475562\n",
      "learning rate 0.000985104546362002\n",
      "\n",
      "======epoch 16 loss====== 0.15567495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 85%|█████████████████████████████████████████████████████████████████████▋            | 17/20 [12:33<02:07, 42.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 16 the mean squared error dev set is 0.21914454057278285\n",
      "learning rate 0.00098411944181564\n",
      "best model updated; new best MSE 0.21914454057278285\n",
      "\n",
      "======epoch 17 loss====== 0.14978503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|█████████████████████████████████████████████████████████████████████████▊        | 18/20 [13:15<01:24, 42.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 17 the mean squared error dev set is 0.21883561469917187\n",
      "learning rate 0.0009831353223738242\n",
      "best model updated; new best MSE 0.21883561469917187\n",
      "\n",
      "======epoch 18 loss====== 0.14397289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 95%|█████████████████████████████████████████████████████████████████████████████▉    | 19/20 [13:58<00:42, 42.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 18 the mean squared error dev set is 0.2179304073495544\n",
      "learning rate 0.0009821521870514505\n",
      "best model updated; new best MSE 0.2179304073495544\n",
      "\n",
      "======epoch 19 loss====== 0.13853054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 20/20 [14:40<00:00, 44.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 19 the mean squared error dev set is 0.21753475288642166\n",
      "learning rate 0.000981170034864399\n",
      "best model updated; new best MSE 0.21753475288642166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# training the CNN model\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "import math\n",
    "best_mse = math.inf\n",
    "best_model = None\n",
    "import copy\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tqdm import tqdm\n",
    "for epoch_i in tqdm(range(n_epochs)):\n",
    "    # the inner loop is over the batches in the dataset\n",
    "    model.train() # let pytorch know that gradients should be computed, so as to update the model\n",
    "    ep_loss = []\n",
    "    for idx in range(0,len(train_user_text),batch_size):\n",
    "        # Step 0: Get the data\n",
    "        sents_u = train_user_text[idx:idx+batch_size]\n",
    "        sents_a = train_arch_text[idx:idx+batch_size]\n",
    "        if (len(sents_u)==0) or (len(sents_a))== 0: break\n",
    "        y_target = torch.tensor([train_labels[idx:idx+batch_size]], dtype=torch.float).squeeze()\n",
    "        if gpu:\n",
    "            y_target = y_target.to('cuda')\n",
    "        \n",
    "        # Step 1: Clear the gradients \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Step 2: Compute the forward pass of the model\n",
    "        y_pred = make_batch_prediction(sents_u,sents_a, word_vectors, model, gpu).squeeze()\n",
    "        #print(y_pred[:5])\n",
    "        #print(y_target[:5])\n",
    "        #pred_labels = [np.argmax(entry) for entry in y_pred.cpu().detach().numpy()]\n",
    "        # print('pred labels', pred_labels)\n",
    "        # print('true labels', y_target)\n",
    "\n",
    "        # Step 3: Compute the loss value that we wish to optimize\n",
    "        loss = loss_fnc(y_pred, y_target)\n",
    "        #loss = loss.type(dtype=torch.long)\n",
    "        # print(loss)\n",
    "        ep_loss.append(loss.cpu().detach().numpy())\n",
    "\n",
    "        # Step 4: Propagate the loss signal backward\n",
    "        loss.backward()\n",
    "        \n",
    "        # Step 4+: clip the gradient, to avoid gradient explosion\n",
    "        nn.utils.clip_grad_value_(model.parameters(), clip_value=3.)\n",
    "\n",
    "        # Step 5: Trigger the optimizer to perform one update\n",
    "        optimizer.step()\n",
    "    \n",
    "    print('\\n======epoch {} loss======'.format(epoch_i),np.mean(ep_loss))\n",
    "    train_loss.append(ep_loss)\n",
    "    \n",
    "    # after each epoch, we can test the model's performance on the dev set\n",
    "    with torch.no_grad(): # let pytorch know that no gradient should be computed\n",
    "        model.eval() # let the model know that it in test mode, i.e. no gradient and no dropout\n",
    "        predictions = []\n",
    "        #test_docs = dev_docs\n",
    "        #test_labels = dev_labels\n",
    "        \n",
    "        for idx in range(0,len(test_user_text),batch_size):\n",
    "            test_uq = test_user_text[idx:idx+batch_size]\n",
    "            test_aq = test_arch_text[idx:idx+batch_size]            \n",
    "            y_pred = make_batch_prediction(test_uq,test_aq, word_vectors, model, gpu)\n",
    "            #pred_labels = [np.argmax(entry) for entry in y_pred.cpu().detach().numpy()] # getting label with highest probability\n",
    "            predictions += y_pred\n",
    "        #print(predictions)\n",
    "        test_labels = [float(i) for i in test_labels]\n",
    "        mse = mean_squared_error(test_labels, predictions)\n",
    "        print('\\n---> after epoch {} the mean squared error dev set is {}'.format(epoch_i, mse))\n",
    "        for param_group in optimizer.param_groups:\n",
    "            print('learning rate', param_group['lr'])\n",
    "        \n",
    "        # save the best model\n",
    "        val_loss.append(mse)\n",
    "        if mse < best_mse:\n",
    "            best_mse = mse\n",
    "            best_model = copy.deepcopy(model.state_dict())\n",
    "            print('best model updated; new best MSE',mse)\n",
    "    \n",
    "    # (optional) adjust learning rate according to the scheduler\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load sample test data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# reconstruct model and make predictions\n",
    "model.load_state_dict(best_model)\n",
    "\n",
    "#testing model\n",
    "test_df_ml = pd.read_csv('test.csv')\n",
    "test_df_ml = test_df_ml[:500]\n",
    "\n",
    "user_questions = test_df_ml['qu'].tolist()\n",
    "reciprocal_rank_list = []\n",
    "arch_questions = test_df_ml['qa'].tolist()\n",
    "l = len(arch_questions)\n",
    "for uquest_id, user_quest in enumerate(user_questions):\n",
    "    most_similar_quest_id = 0\n",
    "    test_user_quest = [user_quest]*l    \n",
    "    y_pred = make_batch_prediction(test_user_quest,arch_questions, word_vectors, model, gpu).cpu()\n",
    "    most_similar_quest_id = np.argmax(np.array(y_pred.detach().numpy()))\n",
    "    #print(most_similar_quest_id)\n",
    "    if arch_questions[uquest_id] ==arch_questions[most_similar_quest_id]:#comparing the retreived with duplicate question of user question\n",
    "        #print(\"\\nMost similar question retrieved\")\n",
    "        reciprocal_rank_list.append(1)\n",
    "    else:\n",
    "        #print(\"\\nDissimilar question retrived\")\n",
    "        reciprocal_rank_list.append(0)\n",
    "    #print(\"User Question:\",user_questions[uquest_id])\n",
    "    #print(\"Archived Question\",arch_questions[uquest_id])\n",
    "    #print(\"Retrieved Question\",arch_questions[most_similar_quest_id])\n",
    "    #if uquest_id==10:\n",
    "        #break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reciprocal Rank for MaLSTM similiarity  0.386\n",
      "Number of question correctly retreived for MaLSTM:  193\n"
     ]
    }
   ],
   "source": [
    "mean_reciprocal_rank = np.mean(np.array(reciprocal_rank_list))\n",
    "print(\"Mean Reciprocal Rank for MaLSTM similiarity \",mean_reciprocal_rank)\n",
    "print(\"Number of question correctly retreived for MaLSTM: \", sum(reciprocal_rank_list) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = train_loss\n",
    "train_loss = [np.mean(x) for x in train_losses]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Loss')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZMAAAEGCAYAAACgt3iRAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy86wFpkAAAACXBIWXMAAAsTAAALEwEAmpwYAAA3mElEQVR4nO3deXgc1ZXw/+9Ra7VWa7EkS7YlG++W5EV4wQQMJKwJW2DAEDuG/MIAIQzJkIEkTxKSvDNvmJD5JRAyQBgbSAAHhiUEDCaExSGAF4x3y8a7Zcm7tdiyrO28f1RJbkktqWWp1d3S+TyPnq6+dW/3UbndR7du1b2iqhhjjDE9ERHsAIwxxoQ/SybGGGN6zJKJMcaYHrNkYowxpscsmRhjjOmxyGAH0JvS09M1Ly8v2GEYY0zY+PTTTw+rakZPX6dfJZO8vDxWrVoV7DCMMSZsiMju3ngdO81ljDGmxyyZGGOM6TFLJsYYY3qsX42ZGGP6j/r6ekpLS6mtrQ12KP1CbGwsubm5REVFBeT1LZkYY0JSaWkpiYmJ5OXlISLBDiesqSpHjhyhtLSU/Pz8gLxHQE9zicilIrJFRLaJyP0+9s8RkUoRWeP+/NjftsaY/q22tpa0tDRLJL1AREhLSwtoLy9gPRMR8QCPAl8CSoGVIvKaqm5qU/XvqvrlM2xrjOnHLJH0nkAfy0Ce5poObFPVHQAishi4CvAnIZxR25ojpax+6t4zDliJoNETTZMnBvXE0uSJafkhMoZGTyzqiUE9MRAVS5MnFiJjwBODRsYinihGDUlgVEbCGcdgjDHhKJDJJAfY6/W8FJjho94sEVkLlAH3qurGbrRFRG4DbgOYlu1h8s4nzzjgCOnZ2i4NGsE7TdN4csp/8d0vjSUjMaZHr2eMCZ4jR45w0UUXAbB//348Hg8ZGc6N4itWrCA6OrrDtqtWreKZZ57h4Ycf9vv9mm+6Tk9P71ngQRLIZOKrT9X223o1MEJVj4vI5cCrwGg/2zqFqk8ATwBMnTZNa36w/IwD1qZGtLEOrTtJU30tWl+LNtRCwyma6mqh4SQ0nGpVLm4Z9bXEHN7Apbve5Hef/p05a8q484Kz+Ma5+cRGec44JmNMcKSlpbFmzRoAHnjgARISErj33tNnPhoaGoiM9P0VWlxcTHFxcV+EGTICmUxKgWFez3Nxeh8tVLXKa3uJiPxORNL9aetLhAgJMT35lSKBGCDxzJqfrID/msCzYz/lO3Uz+OXSLTy3fA/3XTaOrxRm2/lfY8LcggULSE1N5bPPPmPq1KnccMMN3HPPPZw8eZK4uDgWLVrE2LFjef/993nooYd4/fXXeeCBB9izZw87duxgz5493HPPPdx9991+vd/u3bu59dZbOXToEBkZGSxatIjhw4fz4osv8tOf/hSPx0NycjLLli1j48aN3HLLLdTV1dHU1MRLL73E6NGjA3xETgtkMlkJjBaRfGAfcCNwk3cFEckCDqiqish0nKvLjgAVXbUNSXEpMOVrJK5ayJPf+Q8+OpDHz9/YzN3Pf8aif+zkR1+ewNThg4MdpTFh56d/2cimsqquK3bDhKFJ/OQrE7vdbuvWrbzzzjt4PB6qqqpYtmwZkZGRvPPOO/zgBz/gpZdeatempKSE9957j+rqasaOHcsdd9zh1/0ed911F/Pnz+frX/86Cxcu5O677+bVV1/lZz/7GUuXLiUnJ4eKigoAHnvsMf7lX/6Fm2++mbq6OhobG7v9u/VEwC4NVtUG4C5gKbAZeEFVN4rI7SJyu1vtOmCDO2byMHCjOny2DVSsvWrGP0NTA6x8knPOSuf1b5/Lg18tYO/Rk1z7u4+4+/nP2FdxMthRGmPO0PXXX4/H45y6rqys5Prrr2fSpEl85zvfYeNG319TV1xxBTExMaSnpzNkyBAOHDjg13t9/PHH3HST83f0vHnz+PDDDwGYPXs2CxYs4Pe//31L0pg1axb/8R//wYMPPsju3buJi4vr6a/aLQG9aVFVlwBL2pQ95rX9W+C3/rYNC2mjYOxlsGohfOFf8UTFccPZw7micCiPvb+d3/99B0s37uf/+0I+d8w5q4en5YwZGM6kBxEo8fHxLds/+tGPuOCCC3jllVfYtWsXc+bM8dkmJub0xTgej4eGhoYzeu/mU+WPPfYYy5cv54033mDy5MmsWbOGm266iRkzZvDGG29wySWX8OSTT3LhhRee0fucCZubKxBm3gk1R2Ddn1qKEmIiufeSsbx77xwunZTFo+9t54KH3udPK/fQ2NSzq8iMMcFRWVlJTk4OAE899VSvv/4555zD4sWLAXj22Wc599xzAdi+fTszZszgZz/7Genp6ezdu5cdO3YwcuRI7r77bq688krWrVvX6/F0xpJJIOSdC1kF8Ml/g7ZOFDkpcfzmxim8fOc5DBscx30vrefLj3zIR9sOBylYY8yZ+rd/+ze+//3vM3v27F4ZoygsLCQ3N5fc3Fy++93v8vDDD7No0SIKCwv5wx/+wG9+8xsAvve971FQUMCkSZM477zzKCoq4k9/+hOTJk1i8uTJlJSUMH/+/B7H0x2i2n/+Ki4uLtaQWRxrzfPw6u3wtZfhrIt8VlFVXl9Xzi/eLGFfxUm+OD6TH1w+jpF206MxbN68mfHjxwc7jH7F1zEVkU9VtcfXMVvPJFAmXQvxQ+CT33VYRUT4StFQ/vav5/Nvl47lkx1HuPj/X8ZbG8r7MFBjjOk5SyaBEhkD078J296BQ1s6rRob5eHOOWfx3r1zGJIYw8ur9/VRkMYY0zssmQRS8a3giXHGTvyQkRhDcV4q6/dVBjgwY4zpXZZMAik+HYpugLWLoeaoX00Kc5Mpr6zlYLUtCGSMCR+WTAJtxh3OnF6rFvpVvTA3BYD1pdY7McaED0smgZY5AUZeACufhIa6LqtPHJpEhMA6SybGmDBiyaQvzPoWVJfDple7rBofE8lZQxJYV1oR8LCMMR2bM2cOS5cubVX261//mjvvvLPTNr5uT+iovD+xZNIXRl0E6WPg40fb3cToS0FOCuv3VdKf7gEyJtzMnTu35e7zZosXL2bu3LlBiii0WTLpCxERMON2KF8Dez7usnrRsGQOH6+jrNIG4Y0Jluuuu47XX3+dU6dOAbBr1y7Kyso499xzueOOOyguLmbixIn85Cc/OaPXP3r0KFdffTWFhYXMnDmzZfqTDz74gMmTJzN58mSmTJlCdXU15eXlnHfeeUyePJlJkybx97//vdd+z95iswz2laK58O7PnZsYR5zTadWCnGQA1pdWkJPStzN/GhOS3rwf9q/v3dfMKoDLftHh7rS0NKZPn85bb73FVVddxeLFi7nhhhsQEf793/+d1NRUGhsbueiii1i3bh2FhYXdevuf/OQnTJkyhVdffZV3332X+fPns2bNGh566CEeffRRZs+ezfHjx4mNjeWJJ57gkksu4Yc//CGNjY3U1NT09LfvddYz6SvRg2DaLVDyBhzb1WnV8dlJREaIDcIbE2Tep7q8T3G98MILTJ06lSlTprBx40Y2bdrU7df+8MMPmTdvHgAXXnghR44cobKyktmzZ7fMy1VRUUFkZCRnn302ixYt4oEHHmD9+vUkJp7hAn4BZD2TvjT9m/DRw7D8cbj0/3ZYLTbKw9isREsmxjTrpAcRSFdffTXf/e53Wb16NSdPnmTq1Kns3LmThx56iJUrVzJ48GAWLFhAbW33T0n7GhMVEe6//36uuOIKlixZwsyZM3nnnXc477zzWLZsGW+88Qbz5s3je9/7Xp9P5NgV65n0paShMPEaWP0HqO181bjC3BTWlVbYILwxQZSQkMCcOXO49dZbW3olVVVVxMfHk5yczIEDB3jzzTfP6LXPO+88nn32WQDef/990tPTSUpKYvv27RQUFHDfffdRXFxMSUkJu3fvZsiQIXzzm9/kG9/4BqtXr+6137G3WM+kr828E9a/CJ/9EWZ1fIlhYW4yz6/Yw+4jNeSlx3dYzxgTWHPnzuXaa69tOd1VVFTElClTmDhxIiNHjmT27Nl+vc4VV1zRslTvrFmzePzxx7nlllsoLCxk0KBBPP3004Bz+fF7772Hx+NhwoQJXHbZZSxevJhf/vKXREVFkZCQwDPPPBOYX7YHbAr6YFh4KVSVwd2fQYTHZ5UN+yr58iMf8vDcKVxZNLSPAzQm+GwK+t4XtlPQi8ilIrJFRLaJyP2d1DtbRBpF5Dqvsl0isl5E1ohIGGSIbph5J1TsdgbjOzA2K5HoyAjW282LxpgwELBkIiIe4FHgMmACMFdEJnRQ70Fgadt9wAWqOrk3smZIGXcFpAzvdK2TKE8EE7KTWGuD8MaYMBDInsl0YJuq7lDVOmAxcJWPet8GXgIOBjCW0BLhcW5i3PMx7Ot4IK0oN5mN+yptjXgzYPWn0/DBFuhjGchkkgPs9Xpe6pa1EJEc4BrgMR/tFXhbRD4Vkds6ehMRuU1EVonIqkOHDvVC2H1kyjyITux0rZOC3BRO1DWy49DxPgzMmNAQGxvLkSNHLKH0AlXlyJEjxMbGBuw9Ank1l/goa/up+DVwn6o2irSrPltVy0RkCPBXESlR1WXtXlD1CeAJcAbgex52H4lNgqnzYMUT8KWfOpcNt1GY69wJv660ktGZoXeTkjGBlJubS2lpKWH1R2IIi42NJTc3N2CvH8hkUgoM83qeC5S1qVMMLHYTSTpwuYg0qOqrqloGoKoHReQVnNNm7ZJJWJt+m9MzWfF7+GL7+X1GZSQwKNrD+n2VfHVa4D4ExoSiqKgo8vPzgx2G8VMgT3OtBEaLSL6IRAM3Aq95V1DVfFXNU9U84H+BO1X1VRGJF5FEABGJBy4GNgQw1uBIzXcG4z9dBHXt59rxRAiThiaz1q7oMsaEuIAlE1VtAO7CuUprM/CCqm4UkdtF5PYummcCH4rIWmAF8IaqvhWoWINq1rfg5DFYt9jn7sLcZDaVVVHf2NTHgRljjP8Cege8qi4BlrQp8zXYjqou8NreARQFMraQMXwWZE92TndNXeBMV++lIDeZUw1NbD1QzcShyUEJ0RhjumJzcwWbiHMT4+GtsP3ddrttTXhjTDiwZBIKJl4DCVnwyaPtduWlDSIxNpJ1+yyZGGNClyWTUBAZ7UxPv/1dOLi51S4RoTA32daEN8aENEsmoWLaLRAZ63OKlcLcFLbsr6a2vjEIgRljTNcsmYSK+DQouhHW/glOHG61qzAnmfpGpWR/dZCCM8aYzlkyCSUz74TGU85aJ14Kh6UA2AzCxpiQZckklGSMhfSxsOeTVsVDk2NJi4+2ZXyNMSHLkkmoyS6E/etaFYkIBbnJlkyMMSHLkkmoyS6Cqn3tx01yU/j8YDU1dQ1BCswYYzpmySTUZBU6j+VrWxUX5iTTpLCxrCoIQRljTOcsmYSarALnsc2pLu/p6I0xJtRYMgk1g1KdJX3LWyeTIUmxZCXF2hVdxpiQZMkkFGUVtjvNBdggvDEmZFkyCUXZRXB0O5xqfZNiUW4yOw6foKq2PkiBGWOMb5ZMQlHzIPz+1uuBFbgzCG+w3okxJsRYMglF2e5SLj6u6AJsBmFjTMixZBKKErMgPqPdFV2D46MZlhpna5sYY0KOJZNQJOIOwq9rt6swJ8XWhDfGhJyAJhMRuVREtojINhG5v5N6Z4tIo4hc1922/VZ2ERzaDA2nWhUX5iZTeuwkR0/UBSkwY4xpL2DJREQ8wKPAZcAEYK6ITOig3oPA0u627deyC6GpAQ5ualVc0HLzYkUQgjLGGN8C2TOZDmxT1R2qWgcsBq7yUe/bwEvAwTNo23+1TKvS+lRXgTsIb+MmxphQEshkkgPs9Xpe6pa1EJEc4Brgse627fcG50NMUrtB+MTYKEZmxNsVXcaYkBLIZCI+yrTN818D96lq2/Vo/WnrVBS5TURWiciqQ4cOdT/KUBUR4czT5eNO+KLcFDvNZYwJKYFMJqXAMK/nuUBZmzrFwGIR2QVcB/xORK72sy0AqvqEqharanFGRkYvhR4isgrhwEZoap1rC3KSOVB1igNVtUEKzBhjWgtkMlkJjBaRfBGJBm4EXvOuoKr5qpqnqnnA/wJ3quqr/rQdELILob4GjmxrVWwzCBtjQk3AkomqNgB34VyltRl4QVU3isjtInL7mbQNVKwhq4M74ScOTSZCbE14Y0zoiAzki6vqEmBJm7K2g+3N5Qu6ajvgpI8BT4yTTAr/qaU4LtrDmMxEG4Q3xoQMuwM+lHmiIHNCuyu6wDnVta60ElWf1yUYY0yfsmQS6rKLnJ5Jm6RRkJvC0RN17Ks4GaTAjDHmNEsmoS6rEGoroWJPq+KWGYRtEN4YEwIsmYS65kH4Nqe6xmUnEuURSybGmJBgySTUZU4E8bSbViUm0sO4rCS7edEYExIsmYS6qDjnqi4fd8IX5iazfl8lTU02CG+MCS5LJuEgu7DDK7qqaxvYfbQmCEEZY8xplkzCQVYhVJfD8YOtigtyUgCbjt4YE3yWTMJBy53wrXsnYzITiImMsEF4Y0zQWTIJB1kFzuP+1uMmkZ4IJg61QXhjTPBZMgkHcSmQMsL3mvC5KWzYV0WjDcIbY4LIkkm4aL4Tvo3C3GRO1jey/dDxIARljDEOSybhIrsQju107ob30jwd/dq9FUEIyhhjHJZMwkVW853wG1oVj0xPID7aw3qbQdgYE0SWTMJFB9OqREQIk3KSWWtXdBljgsiSSbhIzISETN9rwg9LYXN5FXUNTUEIzBhjLJmEl6xCn1d0FeQkU9fQxNYD1UEIyhhjLJmEl+xCOFQC9bWtiotyUwCbjt4YEzyWTMJJdhFoIxzc2Kp4WGocyXFRrN9XEZy4jDEDXkCTiYhcKiJbRGSbiNzvY/9VIrJORNaIyCoROddr3y4RWd+8L5Bxho2sQuexzakuEaEwN5m1e61nYowJjoAlExHxAI8ClwETgLkiMqFNtb8BRao6GbgVeLLN/gtUdbKqFgcqzrAyOA9ikjucQXjrgWpq6xv7Pi5jzIAXyJ7JdGCbqu5Q1TpgMXCVdwVVPa7asrh5PGBzgnRGxBk38XFFV0FOCg1NyubyqiAEZowZ6AKZTHKAvV7PS92yVkTkGhEpAd7A6Z00U+BtEflURG7r6E1E5Db3FNmqQ4cO9VLoISyrEA5shMaGVsVFw2xNeGNM8AQymYiPsnY9D1V9RVXHAVcDP/faNVtVp+KcJvuWiJzn601U9QlVLVbV4oyMjF4IO8RlF0JDLRz5vFVxVlIs6QkxlkyMMUERyGRSCgzzep4LlHVUWVWXAaNEJN19XuY+HgRewTltZlrWNml9qqt5EN6mozfGBEMgk8lKYLSI5ItINHAj8Jp3BRE5S0TE3Z4KRANHRCReRBLd8njgYqD1pFQDVdpoiIztYDr6ZLYdOs6JUw0+GhpjTOBEBuqFVbVBRO4ClgIeYKGqbhSR2939jwFfBeaLSD1wErhBVVVEMoFX3DwTCTynqm8FKtaw4omEzIkdXtGlChvLqpienxqE4IwxA1XAkgmAqi4BlrQpe8xr+0HgQR/tdgBFgYwtrGUVwoaXQdW5wsvlvSa8JRNjTF+yO+DDUXYRnKqEY7taFWckxjA0OdYG4Y0xfc6SSTjKdu+E93Gqq8AG4Y0xQWDJJBwNmQji6XBN+F1HaqisqQ9CYMaYgcqvZOJeXRXhbo8RkStFJCqwoZkORcVCxjjfa5u4Mwiv2n20j4Myxgxk/vZMlgGxIpKDM5/WLcBTgQrK+CG70OdprrPzB5OeEM3zK/b6aGSMMYHhbzIRVa0BrgUeUdVrcCZvNMGSVQjHD0D1/lbFMZEebjh7GO+WHKD0WE2QgjPGDDR+JxMRmQXcjDOHFgT4smLThZY74dv3Tm6aMQKAZ5fv6cuIjDEDmL/J5B7g+8Ar7o2HI4H3AhaV6VpWgfO4v/24SU5KHF8cn8mfVu61KemNMX3Cr2Siqh+o6pWq+qA7EH9YVe8OcGymM7FJMDjfZ88EYP6sPI6eqOPNDeV9HJgxZiDy92qu50QkyZ0naxOwRUS+F9jQTJc6GIQHmH1WGiMz4nnm4919HJQxZiDy9zTXBFWtwpkmfgkwHJgXqKCMn7KLnLvgT1a02yUizJs5gs/2VLBhn90Rb4wJLH+TSZR7X8nVwJ9VtR5bFTH4stxB+P3rfe6+dmoucVEenvl4V9/FZIwZkPxNJo8Du3CW1l0mIiMAWx822DqZVgUgOS6Kq6fk8Oc1ZVTU1PVhYMaYgcbfAfiHVTVHVS9Xx27gggDHZrqSMAQSs33eCd9s/qwRnGpo4sVVpX0YmDFmoPF3AD5ZRP6rea11EfkVTi/FBFtWYYdXdAGMz07i7LzB/HH5bpqa7MykMSYw/D3NtRCoBv7J/akCFgUqKNMN2YVweAvUdXy3+7xZeew+UsOyzw/1YWDGmIHE32QySlV/oqo73J+fAiMDGZjxU3YRaBMc3NRhlUsnZpGeEMMf7DJhY0yA+JtMTorIuc1PRGQ2zjK7Jtiy3EH4TsZNoiMjmDt9GO9uOcjeozZflzGm9/mbTG4HHhWRXSKyC/gt8M9dNRKRS0Vki4hsE5H7fey/SkTWicgadyzmXH/bGlfKcIhN6fCKrmY3zRhOhAh/XG69E2NM7/P3aq61qloEFAKFqjoFuLCzNiLiAR4FLsOZYXiuiLSdafhvQJGqTgZuBZ7sRlsDzhrwWQWd9kwAspPj+NL4TF6w+bqMMQHQrZUWVbXKvRMe4LtdVJ8ObHPHWOqAxcBVbV7vuKo2X2IUz+kbIbtsa7xkF8GBTdDY+eqK82eN4FhNPW+ss/m6jDG9qyfL9koX+3MA7xWaSt2y1i8ico2IlOBMbX9rd9q67W9rvmT50KEBerVSdhE0noLDWzutNmtUGqMy4nnmEzvVZYzpXT1JJl3dtOAr2bRro6qvqOo4nKlaft6dtm77J1S1WFWLMzIyugipn2oZhO983KR5vq61eytYV1oR+LiMMQNGp8lERKpFpMrHTzUwtIvXLgWGeT3PBco6qqyqy4BRIpLe3bYDXvpoiIzrctwE4NppuQyK9thswsaYXtVpMlHVRFVN8vGTqKpdrbS4EhgtIvkiEg3cCLzmXUFEzhIRcbenAtHAEX/aGi8RHsia1OUVXQBJsVFcMyWHv6wt49gJm6/LGNM7enKaq1Oq2gDcBSwFNgMvuKs03i4it7vVvgpsEJE1OFdv3eDO/eWzbaBi7ReyCp3Zg5uauqw6f1aeM1/Xp3u7rGuMMf6Q0xdThb/i4mJdtWpVsMMIjk+fhr/cDd9eDWmjuqz+T49/zP7KWt6/dw4REV1dS2GM6a9E5FNVLe7p6wSsZ2L6WBfT0bc1b+YI9hyt4YOtA/QKOGNMr7Jk0l8MmQARkV1e0dXskolZZCTG2MJZxpheYcmkv4iMgYxxfl3RBc3zdQ3n/a2H2HPE5usyxvSMJZP+JLvIOc3l5zjYTdNtvi5jTO+wZNKfZBXCiUNQvd+/6smxXDIxkxdW2XxdxpiesWTSn3RzEB7gazNHUFFTz1/W2j2hxpgzZ8mkP8kqcB79HDcBmDUyjdFDEviDzddljOkBSyb9SUwipI7qVjIREebNGsG60krW7K0IXGzGmH7Nkkl/k13YrdNcANdMySE+2mOXCRtjzpglk/4muwgq9kDNUb+bJMZGce3UXF5fV85Rm6/LGHMGLJn0N9lFzuOGl7rVbN6sEdQ1NPHCKpuvyxjTfZZM+pu8L8DIOfDmfVDyht/NxmQmMiM/lT9+spvGpv4zX5sxpm9YMulvPFFww7MwdAq8eAvsXOZ30/mz8ig9dpL3txwMYIDGmP7Ikkl/FJMAN78Iqfnw/FzYt9qvZhdPzCQzKcYWzjLGdJslk/5qUCrMe8V5/ONX4dCWLptEeZz5uj7Yeohdh0/0QZDGmP7Ckkl/ljQU5r3qzCb8h2ugouvB9bnThxMZITxr83UZY7rBkkl/lzbK6aGcOg5/uBqOd75+SWZSLJdMzGLxyr3sPmK9E2OMfyyZDARZk+DmF6ByH/zxWqit7LT6dy8eQ2SEcPOTyymvPNlHQRpjwpklk4Fi+Ey44Y9wcBM8dyPUd5wkRmUk8MytM6ioqedrTy7nyPFTfRioMSYcBTSZiMilIrJFRLaJyP0+9t8sIuvcn49EpMhr3y4RWS8ia0RkgC7s3stGfxGueRz2fAwvLoDG+g6rFuQms3DB2eyrOMn8hSuoPNlxXWOMCVgyEREP8ChwGTABmCsiE9pU2wmcr6qFwM+BJ9rsv0BVJ/fGYvfGVXAdXPEr2PoWvHonNDV1WHV6fiqPfW0aWw9Uc+tTK6mpa+jDQI0x4SSQPZPpwDZV3aGqdcBi4CrvCqr6kaoec59+AuQGMB7T7OxvwIU/gvUvwFv3d7oy45yxQ3j4xil8tucYtz3zqS2iZYzxKZDJJAfwvha11C3ryDeAN72eK/C2iHwqIrd11EhEbhORVSKy6tChzq9UMl6+8K8w6y5Y8Ti8/4tOq15WkM1/XlfEh9sO8+3nP6O+sePejDFmYApkMhEfZT7/BBaRC3CSyX1exbNVdSrOabJvich5vtqq6hOqWqyqxRkZGT2NeeAQgYv/D0z+GnzwC/jksU6rXzctl59eOZG/bjrA915cS5PN32WM8RIZwNcuBYZ5Pc8F2q0NKyKFwJPAZap6pLlcVcvcx4Mi8grOaTP/J5oyXROBr/wGaivgrfsgLgWKbuyw+tfPyeP4qQZ+uXQL8TGR/J+rJyHi628GY8xAE8ieyUpgtIjki0g0cCPwmncFERkOvAzMU9WtXuXxIpLYvA1cDGwIYKwDlycSvvo/kH+eMyBfsqTT6nfOGcXt54/i2eV7+MVbJWgn4y3GmIEjYMlEVRuAu4ClwGbgBVXdKCK3i8jtbrUfA2nA79pcApwJfCgia4EVwBuq+lagYh3womLhxuectVBeXAA7/95hVRHhvkvH8rWZw3n8gx387v3tfRenMSZkSX/6y7K4uFhXrbJbUs5YzVFYdJlzp/yCvzjT2HegqUn51xfX8spn+3jgKxNYMDu/DwM1xvQWEfm0N26/sDvgzWnNMw3HDYZFV8DL/wxb34aG9kv5RkQIv7yukIsnZPLAXzbxoq3QaMyAZsnEtJY01OmVTLwGtr4Jz10PD42G174N29+DxtM3LkZ6Injkpil8YXQ69720jiXry4MYuDEmmOw0l+lYwynY/i5seBm2LIG64xCfAROugonXwvBZEBFBTV0D8/5nBetKK/j9/GLmjB0S7MiNMX7qrdNclkyMf+pPwudvO4ll61JoOAmJ2U4PZuK1VKYVcdOTy9l+6DhP3zKdGSPTgh2xMcYPlkx8sGTSR04dd+b22vAybPsrNNZB8nBOjrmS72wcyYcncnjumzMpzE0JdqTGmC5YMvHBkkkQ1FZCyRtOYtnxHjQ1sFeyeVPP4ZIvX8+IESMhYQjEpjg3SRpjQoolEx8smQRZzVHY/BonP3uR6NKP8HB6Di/1RCPxQ5zE0vKTCfFe283l0QmWeIzpI72VTAI5nYoZaAalwrQFxE1bwMHyvfz1gw9YU7KVxPqjTIyr5ezkenKjq4mo2gdln8GJQ6A+Jo2MGuQM9CdkQmyS8zw6/vRjy/YgiHKft2y3LRvkvKY2tfnR9tuo7zooSEQnP9LF/ghLjqbfs56JCagTpxp4aXUpi/6xi52HT5CZFMP8WXnMnT6c1DiP05s5fgBOHITjB53t417bp6qhvgbqaqD+BNSdgIbaYP9a3ScR4IkGTwxEuo+eKIiMccqbHzvajnTre2Jab7e8Rkfbbdq0TXKIj6ToIzk214vwWGLsZ+w0lw+WTEJXU5Py/taDLPxwFx9uO0xMZATXTs3hltn5jMlM7OaLNbZPMC3bNc7z5u36k8781R1+gXp/kYrvOkD7Xkvb3k1HP+rE29TgXKjQWOdcct12u+GUs/Jl46k2++vcMvd54ynfvbm+JJ7Tyc4T6TxGRLmJy/2JiHL3N5dFQ4RbV8Q5Jtp4+vhok/O8ZbupdXnLc7dNZKzb+0w43Vtt97yT7ag4J54Iz4DvOVoy8cGSSXjYsr+apz7aycur93GqoYkvjE7n1tn5nD8mg4iIgfuf2m9NjW7CcRNQu+TknZTqWyejDk/ntSnzmTxxkmJTvft+boJsqnffxy33Tpzt6rizKUiEk5Saezri8er5eG9HtC8XcXqndc1/SBw/vV1fc4YHVdq8Z0exRZx+7ok83euLjHV6nJGxbk8y1i2PaVPmta+5B9n8Xs2/o3eSa9luLo9oU8cDiZkQm3zGHydLJj5YMgkvR0/U8fyKPTzz8S4OVJ1iZHo8C2bn8dWpucTH2HCeOQMtvVYfiabV9gnfPaNWPaKOekfuPu+k3fzT8rzWSeANtW6Srz2dSHvblY/A1Pln3NySiQ+WTMJTfWMTS9aXs/DDnawtrSQxNpK504czf9YIcgcPCnZ4xvSOpqbWiaWh1klI2uT2+BpPn+prtd3gbjd5bTeXN0HOVEgdecZhWTLxwZJJeFNVVu+pYOE/dvLWhv2oKtPzUzl/zBDOH5PB+OxEW4zLmF5mycQHSyb9R1nFSZ5bvoe/lRxkc3kVAEMSYzhvTAbnj8ngC6PTSRkUHeQojQl/lkx8sGTSPx2oqmXZ1kN8sPUQf//8MJUn64kQKBqWwpwxQzh/bAYFOcl4bPDemG6zZOKDJZP+r7FJWVtawQdbnOSytrQCVRg8KIovjHZ7LWPSGZIYG+xQjQkLlkx8sGQy8Bw9UceH2w63JJfDx08BMHFoEuePyeC8MRlMGZ5CTKQnyJEaE5rCIpmIyKXAbwAP8KSq/qLN/puB+9ynx4E7VHWtP219sWQysDU1KZvKq/jAPSW2evcxGpqUmMgIpg4fzMyRacwYmcrkYSnERllyMQbCIJmIiAfYCnwJKAVWAnNVdZNXnXOAzap6TEQuAx5Q1Rn+tPXFkonxVlVbzyfbj/DJjqMs33mETeVVqEJ0ZARThqUwY2QaM0emMnX4YEsuZsAKh4kepwPbVHUHgIgsBq4CWhKCqn7kVf8TINfftsZ0JSk2iosnZnHxxCwAKmvqWbnrKJ/sOMLynUf57buf8/DfINoTweRhKcwYmcrMkWlMHT6YuGhLLsZ0RyCTSQ6w1+t5KTCjk/rfAN7sblsRuQ24DWD48OFnGqsZAJIHRfHFCZl8cUIm4PRcVu06yvIdToJ59L1tPPLuNqI8QmFuCjNHpjIjP41pIwbbHfnGdCGQ/0N8Xafp85yaiFyAk0zO7W5bVX0CeAKc01zdD9MMVEmxUVw4LpMLxznJpbq2nlW7j7Ukl8c+2MGj720nMkKYlJPMjPxUpuenUpyXSnJcVJCjNya0BDKZlALDvJ7nAmVtK4lIIfAkcJmqHulOW2N6U2JsFBeMHcIFY4cAcPxUA5/uPsbyHUdYuesoi/6xi8eX7UAExmUlMSM/lRn5qZydn0p6QkyQozcmuAI5AB+JM4h+EbAPZxD9JlXd6FVnOPAuMN97/MSftr7YALwJpNr6Rj7bU8GKnUdZsesIn+4+Rm29Mx38qIx4puentfRehqbEBTlaY/wT8gPwqtogIncBS3Eu712oqhtF5HZ3/2PAj4E04HfunEsNqlrcUdtAxWqMP2KjPMwalcasUWnAaOoamthQVukkl51HeX1tGc+v2APAsNQ4puedTi4j0gbZvGKmX7ObFo3pJY1NyubyqpbksmLXUY6ecKYdT4uPpiA3mcKcZApyUyjISSYzKcYSjAm6kO+ZGDPQeNyB+kk5ydx6bj6qyvZDx/lkx1HW7q1g/b5Klm09RJP791tGYoybXJIpzHXa2TQwJlxZMjEmQESEs4YkctaQRL42cwQAJ+sa2VReyfrSStbtcx7f3XKQ5hMEWUmxLT2YSe5jmg3umzBgycSYPhQX7WHaiFSmjUhtKTtxqoGNZVWsK61gwz4nyfx104GW/TkpcRS4PZhJOckU5CSTGm/T75vQYsnEmCCLj4lkujtQ36yqtp6N+6pYv6+C9fucRPPWxv0t+70TTIGbYAZbgjFBZMnEmBCUFBvldeWYo/JkPRv3VbLe7b1s2FdpCcaEDEsmxoSJ5LgozjkrnXPOSm8pq6ypZ0OZk2DW+0gwuYOdBNN8emx8dhIZiTYGY3qfJRNjwljyoChmn5XO7I4STKnz+OaG0wkmPSGG8dmJjMtKZFxWEuOyEzlrSIKt+WJ6xJKJMf1MRwlmY1klm/dXU1Jexeb9VTz98W7qGpw7+CMjhFEZCYzLPp1gxmcl2b0wxm+WTIwZAJIHtT9F1tDYxK4jJ9hcXk3J/ipKyqtZufMof15zehq8wYOiWiWXcdmJjB6SaFP0m3YsmRgzQEV6Ilrug/lK0dCW8sqaeie57HeSzObyahav2MvJ+kYARCAvLZ6xmYluTyaRsVlJDE8dhCfCejEDlSUTY0wryYOimDEyjRkjT19J1tik7DlaQ0m5k2S27K9my4Fqlm7a33LDZWxUBGMyE90kk+QmmUSbUXmAsLm5jDFn7GRdI58frKakvNpJMgeq2LK/msPH61rqpCdEMzYrkbGZSS09mTGZibZUcoiwubmMMUEXF+2hMDeFwtyUVuWHj59iy/5qNpdXtfRinluxu2XK/giB/PR4xmUnMcHtxYzPTiI7OdYG/MOUJRNjTK9LT4gh/ayYVleUeZ8q2+wmmnWlFbyxrrylTlJsZKsEMy47ibGZNuAfDiyZGGP6hCdCyE+PJz89nssKslvKq2vrnV6Mm2BKyqt4cdVeTtSdHvDPT4v3uqLMSTS5g+OsFxNCLJkYY4IqMTaK4rxUivNOz03W1KTsPVbTctny5vIqNpZVsWT96ZsvE2MiW98X4/Zi4mPsay0Y7KgbY0JORIQwIi2eEWnxXDopq6X8+KkGtuw/fV9Myf4qXv1sH9WfNLTUGZE2qOXu/vHZSYzPTmTY4EFE2GXLAWXJxBgTNhJiIpk2YjDTRgxuKVNVSo+ddO6Lce/uLymv5u1NB1ouWx4U7WGsO8g/3h2LGZ+dRIL1YnpNQC8NFpFLgd/grOP+pKr+os3+ccAiYCrwQ1V9yGvfLqAaaMRdG76r97NLg40xzU7WNbL1gDsO447HbC6voqrW6cU0j8VMGJrExKHJTMpxHgfaWjEhf2mwiHiAR4EvAaXAShF5TVU3eVU7CtwNXN3By1ygqocDFaMxpv+Ki/ZQNCyFomEpLWWqSnllbcsYzIZ9lXy2p4LXva4oy06OZeLQZCYOTWJSjvNolyx3LZB9vOnANlXdASAii4GrgJZkoqoHgYMickUA4zDGGMBZSnloShxDU+K4aHxmS3lFTR0by6rYWFbZkmT+VnL6NNngQVFMyklmwtAkJrmJJi8t3sZhvAQymeQAe72elwIzutFegbdFRIHHVfWJ3gzOGGOapQyKbjfTck1dA5vLq50Es6+KDWWVLPxwJ/WNToaJj/YwPjuJiUOTWk6Vjc4cuFP5BzKZ+ErZ3Rmgma2qZSIyBPiriJSo6rJ2byJyG3AbwPDhw88sUmOMaWNQdPvB/rqGJj4/WO30YvZVsqm8iv/9tJQTHzv3xERGCKMzE5ngJpnmRJMYGxWsX6PPBDKZlALDvJ7nAmUd1G1HVcvcx4Mi8grOabN2ycTtsTwBzgB8TwI2xpjOREdGuOMpyVDsfL01NSm7j9a0nCLbVFbFB1sP8dLq0pZ2I9IGeSUY5zRZRmL/WismkMlkJTBaRPKBfcCNwE3+NBSReCBCVavd7YuBnwUsUmOMOUMRXnf2f7nw9FT+B6tqW8ZhNrkD/q1XvIxmfPO0Mf1gxcuAJRNVbRCRu4ClOJcGL1TVjSJyu7v/MRHJAlYBSUCTiNwDTADSgVfcrB0JPKeqbwUqVmOM6W1DkmIZkhTLBeOGtJRV1dazuazKTTJVlLRZ8dITIYxMj2+5J6Z5Gv+clNCfOsamoDfGmCBqXvHSuemyumVhstJjJ1vqJMZGtiSWcVmnk0xvjMX01n0mlkyMMSYEVdfWuzddVreaQqb61OmpY3IHx/GDy8dzudfEmd0V8jctGmOMOXOJsVFMG5HKtBGnJ8BUVcoqa1tWvCzZXx0yK1laMjHGmDAhIuSkxJHT5qbLUBAR7ACMMcaEP0smxhhjesySiTHGmB6zZGKMMabHLJkYY4zpMUsmxhhjesySiTHGmB6zZGKMMabH+tV0KiJSDWwJdhxdSAfCYSlii7N3WZy9y+LsPWNVNbGnL9Lf7oDf0htzzASSiKwK9RjB4uxtFmfvsjh7j4j0yoSGdprLGGNMj1kyMcYY02P9LZk8EewA/BAOMYLF2dsszt5lcfaeXomxXw3AG2OMCY7+1jMxxhgTBJZMjDHG9FjYJRMRuVREtojINhG538d+EZGH3f3rRGRqEGIcJiLvichmEdkoIv/io84cEakUkTXuz4/7Ok43jl0ist6Nod0lgiFyPMd6Hac1IlIlIve0qROU4ykiC0XkoIhs8CpLFZG/isjn7uPgDtp2+lnugzh/KSIl7r/rKyKS0kHbTj8jfRDnAyKyz+vf9vIO2vbJ8ewgxj95xbdLRNZ00LYvj6XP76GAfT5VNWx+AA+wHRgJRANrgQlt6lwOvAkIMBNYHoQ4s4Gp7nYisNVHnHOA10PgmO4C0jvZH/Tj6eMzsB8YEQrHEzgPmAps8Cr7T+B+d/t+4MEOfo9OP8t9EOfFQKS7/aCvOP35jPRBnA8A9/rxueiT4+krxjb7fwX8OASOpc/voUB9PsOtZzId2KaqO1S1DlgMXNWmzlXAM+r4BEgRkey+DFJVy1V1tbtdDWwGcvoyhl4U9OPZxkXAdlXdHcQYWqjqMuBom+KrgKfd7aeBq3009eezHNA4VfVtVW1wn34C5Abq/f3VwfH0R58dz85iFBEB/gl4PhDv3R2dfA8F5PMZbskkB9jr9byU9l/S/tTpMyKSB0wBlvvYPUtE1orImyIysW8ja6HA2yLyqYjc5mN/SB1P4EY6/o8aCscTIFNVy8H5Dw0M8VEn1I7rrTg9UF+6+oz0hbvc03ELOzgtEyrH8wvAAVX9vIP9QTmWbb6HAvL5DLdkIj7K2l7b7E+dPiEiCcBLwD2qWtVm92qcUzVFwCPAq30cXrPZqjoVuAz4loic12Z/KB3PaOBK4EUfu0PlePorlI7rD4EG4NkOqnT1GQm0/wZGAZOBcpzTSG2FyvGcS+e9kj4/ll18D3XYzEdZp8cz3JJJKTDM63kuUHYGdQJORKJw/gGfVdWX2+5X1SpVPe5uLwGiRCS9j8NEVcvcx4PAKzjdW28hcTxdlwGrVfVA2x2hcjxdB5pPBbqPB33UCYnjKiJfB74M3KzuyfK2/PiMBJSqHlDVRlVtAn7fwfsH/XiKSCRwLfCnjur09bHs4HsoIJ/PcEsmK4HRIpLv/pV6I/BamzqvAfPdq5BmApXNXbq+4p43/R9gs6r+Vwd1stx6iMh0nH+LI30XJYhIvIgkNm/jDMhuaFMt6MfTS4d/9YXC8fTyGvB1d/vrwJ991PHnsxxQInIpcB9wparWdFDHn89IQLUZo7umg/cP+vEEvgiUqGqpr519fSw7+R4KzOezL64q6OUrFC7HuSphO/BDt+x24HZ3W4BH3f3rgeIgxHguTpdwHbDG/bm8TZx3ARtxrpL4BDgnCHGOdN9/rRtLSB5PN45BOMkh2ass6McTJ7mVA/U4f819A0gD/gZ87j6munWHAks6+yz3cZzbcM6LN39GH2sbZ0efkT6O8w/uZ28dzhdadjCPp68Y3fKnmj+PXnWDeSw7+h4KyOfTplMxxhjTY+F2mssYY0wIsmRijDGmxyyZGGOM6TFLJsYYY3rMkokxxpges2Riwo6IqIj8yuv5vSLyQC+99lMicl1vvFYX73O9O5vre4F+rzbvu0BEftuX72kGBksmJhydAq4N4h3uPomIpxvVvwHcqaoXBCoeY/qSJRMTjhpw1q3+TtsdbXsWInLcfZwjIh+IyAsislVEfiEiN4vICnd9iVFeL/NFEfm7W+/LbnuPOOt/rHQnHPxnr9d9T0Sew7mxrm08c93X3yAiD7plP8a5oewxEfmljzbf83qfn7pleeKsPfK0W/6/IjLI3XeRiHzmvs9CEYlxy88WkY/EmfxyRfPd18BQEXlLnPUs/tPr93vKjXO9iLQ7tsZ0JjLYARhzhh4F1jV/GfqpCBiPM334DuBJVZ0uzqJB3wbucevlAefjTC74noicBczHmUrmbPfL+h8i8rZbfzowSVV3er+ZiAzFWSdkGnAMZ7bYq1X1ZyJyIc4aHavatLkYGO2+pgCviTMZ4B5gLM7d1v8QkYXAne4pq6eAi1R1q4g8A9whIr/DmSPqBlVdKSJJwEn3bSbjzCB7CtgiIo/gzBybo6qT3DhSunFcjbGeiQlP6sx++gxwdzearVRnjYdTOFNENCeD9TgJpNkLqtqkzjTiO4BxOPMozRdnBb3lOFNSjHbrr2ibSFxnA++r6iF11g15Fmdhpc5c7P58hjMT8jiv99mrqv9wt/+I07sZC+xU1a1u+dPue4wFylV1JbRMhNm8dsnfVLVSVWuBTcAI9/ccKSKPuHN2+Tu7rDGA9UxMePs1zhfuIq+yBtw/ktyJ7qK99p3y2m7yet5E6/8LbecYUpxewrdVdan3DhGZA5zoID5f03h3RYD/q6qPt3mfvE7i6uh1Oporyfs4NOKstnhMRIqAS4Bv4SzwdGv3QjcDmfVMTNhS1aPACziD2c124ZxWAmdluKgzeOnrRSTCHUcZCWwBluKcPooCEJEx7syvnVkOnC8i6e7g/Fzggy7aLAVuFWcNCkQkR0SaFy8aLiKz3O25wIdACZDnnooDmOe+RwnO2MjZ7uskijNFuk/uxQwRqvoS8COcZWmN8Zv1TEy4+xXOjMHNfg/8WURW4MyI2lGvoTNbcL6QM3Fmga0VkSdxToWtdns8h/C93GkLVS0Xke8D7+H0FJaoqq/pvr3bvC0i44GPnbfhOPA1nB7EZuDrIvI4zoyv/+3GdgvwopssVuLM/lsnIjcAj4hIHM54yRc7eescYJGINP+B+f3O4jSmLZs12Jgw4J7mer15gNyYUGOnuYwxxvSY9UyMMcb0mPVMjDHG9JglE2OMMT1mycQYY0yPWTIxxhjTY5ZMjDHG9Nj/A0SXvmIcHrW/AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import  matplotlib.pyplot as plt\n",
    "plt.plot(train_loss,label = \"Train Loss\")\n",
    "plt.plot(val_loss,label = \"Val Loss\")\n",
    "plt.xlim(0,20)\n",
    "plt.legend()\n",
    "#plt.legend(\"Train Loss\",\"Val Loss\")\n",
    "plt.xlabel(\"Number of epochs\")\n",
    "plt.ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reference Gao,Yang, LSTM-Lab9 ,Royal Holloway, University Of London "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vevn_nlp",
   "language": "python",
   "name": "vevn_nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
